{
    "preguntas": [
        {
            "id": 1,
            "pregunta": "¿Qué es Hadoop y cuáles son sus principales módulos?",
            "respuesta": "Hadoop es un marco de trabajo de código abierto escrito en Java que permite el almacenamiento y procesamiento distribuido de grandes conjuntos de datos. Sus principales módulos son: Hadoop Common, Hadoop Distributed File System (HDFS), Hadoop YARN y Hadoop MapReduce."
        },
        {
            "id": 2,
            "pregunta": "¿Qué diferencias clave existen entre Hadoop V1, Hadoop V2 y Hadoop V3?",
            "respuesta": "En Hadoop V1, MapReduce gestionaba tanto la administración de recursos como la ejecución de trabajos. Name Node era el unico punto de fallo. Era costoso iniciar el Node Secondary. Requeriria intervencion del sysadmin para actuar. En Hadoop V2, se introdujo YARN, que separa la gestión de recursos del procesamiento, permitiendo que otros proyectos, además de MapReduce, se ejecuten en Hadoop. Se introdujo HDFS federation y HDFS high availability. Se introdujo la capacidad de reiniciar el NameNode sin intervencion del sysadmin. La recuperacion del sistema en segundos. En V3 se introdujo el erasure coding."
        },
        {
            "id": 3,
            "pregunta": "Explica la arquitectura maestro/trabajador en Hadoop.",
            "respuesta": "Hadoop sigue una arquitectura maestro/trabajador donde un nodo maestro (NameNode en HDFS o Resource Manager en YARN) gestiona los recursos y la planificación, mientras que los nodos trabajadores (DataNodes en HDFS o Node Managers en YARN) ejecutan las tareas asignadas y almacenan los datos."
        },
        {
            "id": 4,
            "pregunta": "¿Qué es el modelo de programación MapReduce?",
            "respuesta": "MapReduce es un modelo de programación diseñado para procesar grandes volúmenes de datos distribuidos en varios nodos. Consiste en dos fases principales: 'map', que transforma los datos de entrada en pares clave-valor, y 'reduce', que agrega y procesa estos pares."
        },
        {
            "id": 5,
            "pregunta": "¿Cómo se organiza y ejecuta un proceso de MapReduce en Hadoop?",
            "respuesta": "El proceso se organiza en dos etapas: la fase 'map', que divide los datos y los transforma en pares clave-valor, y la fase 'reduce', que toma estos pares y los agrupa para realizar un procesamiento adicional. El proceso es coordinado por YARN en Hadoop V2."
        },
        {
            "id": 6,
            "pregunta": "¿Cuál es el propósito de las funciones de map y reduce en el contexto de Hadoop?",
            "respuesta": "La función 'map' transforma los datos de entrada en pares clave-valor, mientras que la función 'reduce' toma estos pares, los agrupa por clave y aplica una operación de agregación para producir un resultado consolidado."
        },
        {
            "id": 7,
            "pregunta": "¿Qué es YARN y qué papel desempeña en Hadoop?",
            "respuesta": "YARN (Yet Another Resource Negotiator) es un sistema de gestión de recursos en Hadoop que permite la ejecución de múltiples aplicaciones distribuidas, incluida pero no limitada a MapReduce. Coordina el uso de recursos como CPU y memoria en el clúster."
        },
        {
            "id": 8,
            "pregunta": "¿Cuáles son los cuatro conceptos clave alrededor de los que se organiza YARN?",
            "respuesta": "Los cuatro conceptos clave en YARN son: Resource Manager (gestiona los recursos globales), Node Manager (gestiona los recursos locales en los nodos), Container (subconjunto de recursos asignados) y Application Master (gestiona la ejecución de una aplicación específica)."
        },
        {
            "id": 9,
            "pregunta": "¿Qué ventajas aporta la inclusión de YARN en la versión 2 de Hadoop?",
            "respuesta": "YARN permite que Hadoop ejecute múltiples aplicaciones distribuidas más allá de MapReduce, optimizando el uso de recursos y proporcionando mayor flexibilidad en la ejecución de trabajos. Esto también mejora el rendimiento y la escalabilidad."
        },
        {
            "id": 10,
            "pregunta": "¿Qué es el sistema de archivos distribuidos de Hadoop (HDFS)?",
            "respuesta": "HDFS es un sistema de archivos distribuido que almacena grandes volúmenes de datos en un clúster de nodos, asegurando la tolerancia a fallos mediante la replicación de los bloques de datos en múltiples nodos."
        },
        {
            "id": 11,
            "pregunta": "¿Qué características hacen que HDFS sea adecuado para procesar Big Data?",
            "respuesta": "HDFS maneja grandes volúmenes de datos distribuidos, con soporte para tolerancia a fallos mediante replicación. Está diseñado para acceso secuencial a datos y prioriza el alto rendimiento sobre la baja latencia, lo que lo hace ideal para análisis de Big Data."
        },
        {
            "id": 12,
            "pregunta": "¿Cuáles son los mecanismos de tolerancia a fallos implementados en HDFS?",
            "respuesta": "HDFS implementa la tolerancia a fallos mediante la replicación de datos, replicando cada bloque en al menos tres nodos. También incluye un nodo secundario (Secondary NameNode) para respaldar al NameNode en caso de fallo."
        },
        {
            "id": 13,
            "pregunta": "¿Cuáles son algunas de las limitaciones de Hadoop MapReduce?",
            "respuesta": "MapReduce tiene una programación engorrosa y es ineficiente para aplicaciones que requieren múltiples pasos, como algoritmos iterativos o consultas interactivas. Además, la serialización de datos entre etapas y el uso intensivo del disco generan cuellos de botella."
        },
        {
            "id": 14,
            "pregunta": "Menciona una alternativa a MapReduce que sea mejor para el procesamiento iterativo o consultas interactivas.",
            "respuesta": "Apache Spark es una alternativa a MapReduce que es más eficiente para aplicaciones iterativas y consultas interactivas debido a su capacidad de mantener los datos en memoria durante el procesamiento, lo que reduce el tiempo de ejecución."
        },
        {
            "id": 15,
            "pregunta": "¿Qué es el ecosistema de Hadoop y qué herramientas lo componen?",
            "respuesta": "El ecosistema de Hadoop incluye un conjunto de herramientas para procesamiento y gestión de datos, como Apache Hive (consultas SQL), Apache Pig (scripts para datos), Apache HBase (almacenamiento NoSQL), Apache Sqoop (importación/exportación de datos) y Apache Flume (ingesta de datos)."
        },
        {
            "id": 16,
            "pregunta": "¿Cómo se diferencian Apache Hive, Apache Pig y Apache HBase en sus propósitos dentro del ecosistema Hadoop?",
            "respuesta": "Apache Hive proporciona un lenguaje SQL para consultar grandes conjuntos de datos, Apache Pig permite crear scripts para transformar datos y Apache HBase es una base de datos NoSQL que almacena grandes cantidades de datos en tablas distribuidas."
        },
        {
            "id": 17,
            "pregunta": "Explica cómo se lee un archivo desde HDFS.",
            "respuesta": "Para leer un archivo desde HDFS, el cliente solicita al NameNode la ubicación de los bloques de datos. Luego, el cliente lee directamente desde los DataNodes más cercanos que contienen los bloques de datos requeridos."
        },
        {
            "id": 18,
            "pregunta": "Describe el proceso de escritura de un archivo en HDFS.",
            "respuesta": "El cliente solicita al NameNode crear un archivo en el sistema. El cliente empieza a escribir datos en un DataNode, que replica los bloques en otros DataNodes asignados. El NameNode confirma la finalización una vez que todas las réplicas están completas."
        },
        {
            "id": 19,
            "pregunta": "¿Por qué HDFS utiliza bloques de tamaño grande (64 MB o más) en comparación con sistemas de archivos tradicionales?",
            "respuesta": "HDFS utiliza bloques grandes para minimizar el tiempo de búsqueda (seek time) durante la lectura de datos y optimizar el rendimiento de I/O al procesar grandes volúmenes de información en sistemas distribuidos."
        },
        {
            "id": 20,
            "pregunta": "¿Cuáles son las principales desventajas de HDFS para el acceso a archivos pequeños?",
            "respuesta": "HDFS no es eficiente para el acceso a muchos archivos pequeños, ya que cada archivo tiene metadatos almacenados en el NameNode, lo que puede sobrecargar su memoria. Además, el sistema está optimizado para operaciones de lectura secuencial y no para acceso aleatorio."
        },
        {
            "id": 21,
            "pregunta": "Nombra los cuatro modulos de Hadoop",
            "respuesta": "Hadoop Common, Hadoop Distributed File System (HDFS), Hadoop YARN y Hadoop MapReduce."
        },
        {
            "id": 22,
            "pregunta": "Nombra los cuatro conceptos clave de YARN",
            "respuesta": "Resource Manager, Node Manager, Container y Application Master."

        },
        {
            "id": 23,
               "pregunta": "Ventajas de usar el bloque en almacenamiento de HDFS",
               "respuesta": "Los bloques de gran tamaño permiten una lectura secuencial eficiente, reduciendo el tiempo de búsqueda y optimizando el rendimiento de I/O en sistemas distribuidos. Permite que se almnacen grandes volúmenes de información en sistemas distribuidos. Permite la separacion de metadatos y bloques de datos. Facilita la replicacion y ayuda a gestionar el espacio libre."
        }
    ]
}
